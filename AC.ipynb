{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement actor critic algorithm for Catch environment from scratch using pytorch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from catch import Catch\n",
    "import argparse\n",
    "import gym\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define the actor critic network architecture\n",
    "# class ActorCritic(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(ActorCritic, self).__init__()\n",
    "#         self.affine1 = nn.Linear(2, 128)\n",
    "\n",
    "#         # actor's layer\n",
    "#         self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "#         # critic's layer\n",
    "#         self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "#         # action & reward buffer\n",
    "#         self.saved_actions = []\n",
    "#         self.rewards = []\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         forward of both actor and critic\n",
    "#         \"\"\"\n",
    "#         x = F.relu(self.affine1(x))\n",
    "\n",
    "#         # actor: choses action to take from state s_t\n",
    "#         # by returning probability of each action\n",
    "#         action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "#         # critic: evaluates being in the state s_t\n",
    "#         state_values = self.value_head(x)\n",
    "\n",
    "#         # return values for both actor and critic as a tuple of 2 values:\n",
    "#         # 1. a list with the probability of each action over the action space\n",
    "#         # 2. the value from state s_t\n",
    "#         return action_prob, state_values\n",
    "\n",
    "\n",
    "# model = ActorCritic()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "# eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Actor and Critic networks architecture\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, gamma, lr, trace_length, device, use_baseline=False, use_bootstrapping=False, entropy_strength=0.01):\n",
    "        self.actor = Actor(input_size, output_size, hidden_size).to(device)\n",
    "        self.critic = Critic(input_size, hidden_size).to(device)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.use_baseline = use_baseline\n",
    "        self.use_bootstrapping = use_bootstrapping\n",
    "        self.entropy_weight = entropy_strength\n",
    "        self.trace_length = trace_length\n",
    "\n",
    "    # def get_action(self, state):\n",
    "    #     state = torch.FloatTensor(state).to(self.device)\n",
    "    #     policy_probs = self.actor(state)\n",
    "    #     action = torch.distributions.Categorical(policy_probs).sample()\n",
    "    #     return action.item()\n",
    "\n",
    "    def train(self, episodes):\n",
    "        env = Catch(grid_size=7)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            log_probs = []\n",
    "\n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                policy_probs = self.actor(state)\n",
    "                action_dist = torch.distributions.Categorical(policy_probs)\n",
    "                action = action_dist.sample()\n",
    "                log_prob = action_dist.log_prob(action)\n",
    "\n",
    "                next_state, reward, done = env.step(action.item())\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Compute advantages\n",
    "            values = self.critic(torch.stack(states).to(self.device)).squeeze()\n",
    "            next_values = torch.cat((values[1:], torch.zeros(1).to(self.device)))\n",
    "            delta = rewards + self.gamma * next_values - values\n",
    "            advantages = self.compute_advantages(env, delta)\n",
    "\n",
    "            # Compute policy loss\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            policy_loss = -torch.mean(log_probs * advantages) - self.entropy_strength * torch.mean(policy_probs * torch.log(policy_probs))\n",
    "\n",
    "            # Compute value loss\n",
    "            targets = rewards + self.gamma * next_values\n",
    "            value_loss = nn.MSELoss()(values, targets)\n",
    "\n",
    "            # Compute total loss\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # Update actor and critic networks\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_actor.step()\n",
    "            self.optimizer_critic.step()\n",
    "            \n",
    "    def compute_advantages(self, env, delta):\n",
    "        if self.use_baseline:\n",
    "            delta -= torch.mean(delta)\n",
    "        if self.use_bootstrapping:\n",
    "            delta += self.gamma * self.critic(torch.FloatTensor(env.reset()).to(self.device)).squeeze()\n",
    "        return delta\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfe08b29da77ff4049069357aeebf498efee16b7c76a4fcdec5e710ea9b27a43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
